{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805b37c1-5680-4b83-88fe-2a2855d4b13b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Daylength\n",
    "import numpy as np\n",
    "\n",
    "def day_length(day_of_year, latitude):\n",
    "    # Ensure arrays are numpy arrays for vectorized operations\n",
    "    day_of_year = np.array(day_of_year)\n",
    "    latitude_rad = np.deg2rad(latitude)\n",
    "    \n",
    "    P = np.arcsin(0.39795 * np.cos(0.2163108 + 2 * np.arctan(0.9671396 * np.tan(0.00860 * (day_of_year - 186)))))\n",
    "    \n",
    "    numerator = np.sin(np.deg2rad(0.8333)) + np.sin(latitude_rad) * np.sin(P)\n",
    "    denominator = np.cos(latitude_rad) * np.cos(P)\n",
    "    acos_arg = numerator / denominator\n",
    "\n",
    "    # Clip the value to avoid NaN due to domain errors in arccos\n",
    "    acos_arg = np.clip(acos_arg, -1.0, 1.0)\n",
    "\n",
    "    day_light_hours = 24 - (24 / np.pi) * np.arccos(acos_arg)\n",
    "    return day_light_hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c2777d-9750-4723-b3ee-0bedc5dfaeda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to snap to nearest value in array\n",
    "def snap_to_array(values, ref_array):\n",
    "    idx = np.abs(values[:, None] - ref_array).argmin(axis=1)\n",
    "    return ref_array[idx]\n",
    "\n",
    "latitude = np.array([30.62827225, 31.57068063, 32.51308901, 33.45549738, 34.39790576, 35.34031414,\n",
    " 36.28272251, 37.22513089, 38.16753927, 39.10994764, 40.05235602, 40.9947644,\n",
    " 41.93717277, 42.87958115, 43.82198953, 44.76439791, 45.70680628, 46.64921466,\n",
    " 47.59162304, 48.53403141, 49.47643979, 50.41884817, 51.36125654, 52.30366492,\n",
    " 53.2460733,  54.18848168, 55.13089005, 56.07329843, 57.01570681, 57.95811518,\n",
    " 58.90052356, 59.84293194, 60.78534031, 61.72774869, 62.67015707, 63.61256545,\n",
    " 64.55497382, 65.4973822,  66.43979058, 67.38219895, 68.32460733, 69.26701571,\n",
    " 70.20942408, 71.15183246, 72.09424084, 73.03664921, 73.97905759, 74.92146597,\n",
    " 75.86387435, 76.80628272, 77.7486911,  78.69109948, 79.63350785, 80.57591623,\n",
    " 81.51832461, 82.46073298, 83.40314136, 84.34554974, 85.28795812, 86.23036649,\n",
    " 87.17277487, 88.11518325, 89.05759162, 90.\n",
    "])\n",
    "longitude = np.array([  0.,     1.25,   2.5,    3.75,   5.,     6.25,   7.5,    8.75,  10.,    11.25,\n",
    "  12.5,   13.75,  15.,    16.25,  17.5,   18.75,  20.,    21.25,  22.5,   23.75,\n",
    "  25.,    26.25,  27.5,   28.75,  30.,    31.25,  32.5,   33.75,  35.,    36.25,\n",
    "  37.5,   38.75,  40.,    41.25,  42.5,   43.75,  45.,    46.25,  47.5,   48.75,\n",
    "  50.,    51.25,  52.5,   53.75,  55.,    56.25,  57.5,   58.75,  60.,    61.25,\n",
    "  62.5,   63.75,  65.,    66.25,  67.5,   68.75,  70.,    71.25,  72.5,   73.75,\n",
    "  75.,    76.25,  77.5,   78.75,  80.,    81.25,  82.5,   83.75,  85.,    86.25,\n",
    "  87.5,   88.75,  90.,    91.25,  92.5,   93.75,  95.,    96.25,  97.5,   98.75,\n",
    " 100.,   101.25, 102.5,  103.75, 105.,   106.25, 107.5,  108.75, 110.,   111.25,\n",
    " 112.5,  113.75, 115.,   116.25, 117.5,  118.75, 120.,   121.25, 122.5,  123.75,\n",
    " 125.,   126.25, 127.5,  128.75, 130.,   131.25, 132.5,  133.75, 135.,   136.25,\n",
    " 137.5,  138.75, 140.,   141.25, 142.5,  143.75, 145.,   146.25, 147.5,  148.75,\n",
    " 150.,   151.25, 152.5,  153.75, 155.,   156.25, 157.5,  158.75, 160.,   161.25,\n",
    " 162.5,  163.75, 165.,   166.25, 167.5,  168.75, 170.,   171.25, 172.5,  173.75,\n",
    " 175.,   176.25, 177.5,  178.75, 180.,   181.25, 182.5,  183.75, 185.,   186.25,\n",
    " 187.5,  188.75, 190.,   191.25, 192.5,  193.75, 195.,   196.25, 197.5,  198.75,\n",
    " 200.,   201.25, 202.5,  203.75, 205.,   206.25, 207.5,  208.75, 210.,   211.25,\n",
    " 212.5,  213.75, 215.,   216.25, 217.5,  218.75, 220.,   221.25, 222.5,  223.75,\n",
    " 225.,   226.25, 227.5,  228.75, 230.,   231.25, 232.5,  233.75, 235.,   236.25,\n",
    " 237.5,  238.75, 240.,   241.25, 242.5,  243.75, 245.,   246.25, 247.5,  248.75,\n",
    " 250.,   251.25, 252.5,  253.75, 255.,   256.25, 257.5,  258.75, 260.,   261.25,\n",
    " 262.5,  263.75, 265.,   266.25, 267.5,  268.75, 270.,   271.25, 272.5,  273.75,\n",
    " 275.,   276.25, 277.5,  278.75, 280.,   281.25, 282.5,  283.75, 285.,   286.25,\n",
    " 287.5,  288.75, 290.,   291.25, 292.5,  293.75, 295.,   296.25, 297.5,  298.75,\n",
    " 300.,   301.25, 302.5,  303.75, 305.,   306.25, 307.5,  308.75, 310.,   311.25,\n",
    " 312.5,  313.75, 315.,   316.25, 317.5,  318.75, 320.,   321.25, 322.5,  323.75,\n",
    " 325.,   326.25, 327.5,  328.75, 330.,   331.25, 332.5,  333.75, 335.,   336.25,\n",
    " 337.5,  338.75, 340.,   341.25, 342.5,  343.75, 345.,   346.25, 347.5,  348.75,\n",
    " 350.,   351.25, 352.5,  353.75, 355.,   356.25, 357.5,  358.75])\n",
    "longitude = longitude - 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d3395-8d19-431c-b31a-e300b30444d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## CDD\n",
    "def CDD_model(par, data):\n",
    "    if len(par) != 2:\n",
    "        raise ValueError(\"model parameter(s) out of range (too many, too few)\")\n",
    "\n",
    "    T_base, F_crit = par\n",
    "    Tmini = data['Tmini']  # shape: (366, num_sites)\n",
    "    # Calculate chilling rate (Rf)\n",
    "    Rf = Tmini - T_base\n",
    "    Rf[Rf > 0] = 0\n",
    "\n",
    "    # Determine start day (t0) for chilling accumulation after day 200\n",
    "    t0 = []\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        temp_series = Tmini[:, col]\n",
    "        below_base = np.where(temp_series < T_base)[0]\n",
    "        later_days = below_base[below_base > 173]\n",
    "        if len(later_days) == 0:\n",
    "            t0.append(174)  # Default to day 201 if none found\n",
    "        else:\n",
    "            t0.append(later_days[0])\n",
    "\n",
    "    # Nullify values before t0\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        Rf[:t0[col], col] = 0\n",
    "\n",
    "    # Compute doy when cumulative Rf exceeds F_crit\n",
    "    doy = []\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        valid_days = np.where(cumulative <= F_crit)[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    return np.array(doy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca4bb6-839f-4e92-acae-8e36738c2efa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## DM\n",
    "def DM_model(par, data):\n",
    "    if len(par) != 3:\n",
    "        raise ValueError(\"model parameter(s) out of range (too many, too few)\")\n",
    "\n",
    "    T_base, P_base, F_crit = par\n",
    "    Tmini = data['Tmini']      # shape: (days, sites)\n",
    "    Li = data['Li']            # shape: (days, sites)\n",
    "    \n",
    "    # Rate function: only negative values contribute\n",
    "    Rf = (Tmini - T_base) * (Li / P_base)\n",
    "\n",
    "    Rf[Rf > 0] = 0\n",
    "\n",
    "    # Determine start day (t0) for chilling accumulation after day 200\n",
    "    t0 = []\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        temp_series = Tmini[:, col]\n",
    "        below_base = np.where(temp_series < T_base)[0]\n",
    "        later_days = below_base[below_base > 173]\n",
    "        if len(later_days) == 0:\n",
    "            t0.append(174)  # Default to day 201 if none found\n",
    "        else:\n",
    "            t0.append(later_days[0])\n",
    "\n",
    "    # Nullify values before t0\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        Rf[:t0[col], col] = 0\n",
    "\n",
    "    # Compute doy when cumulative Rf exceeds F_crit\n",
    "    doy = []\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        valid_days = np.where(cumulative <= F_crit)[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    return np.array(doy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbd639-117c-41c3-8829-38d59d13853a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SIAM\n",
    "def SIAM_model(par, predictor, data):\n",
    "    if len(par) != 4:\n",
    "        raise ValueError(\"model parameter(s) out of range (too many, too few)\")\n",
    "\n",
    "    T_base, P_base, a, b = par\n",
    "    Tmini = data[\"Tmini\"]\n",
    "    Li = data[\"Li\"]\n",
    "\n",
    "    Rf = (Tmini - T_base) * (Li / P_base)\n",
    "    Rf[Rf > 0] = 0\n",
    "\n",
    "    # Determine start day (t0) for chilling accumulation after day 200\n",
    "    t0 = []\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        temp_series = Tmini[:, col]\n",
    "        below_base = np.where(temp_series < T_base)[0]\n",
    "        later_days = below_base[below_base > 173]\n",
    "        if len(later_days) == 0:\n",
    "            t0.append(174)  # Default to day 201 if none found\n",
    "        else:\n",
    "            t0.append(later_days[0])\n",
    "\n",
    "    # Nullify values before t0\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        Rf[:t0[col], col] = 0\n",
    "\n",
    "    # Compute doy when cumulative Rf exceeds F_crit\n",
    "    doy = []\n",
    "    # print(Rf.shape[1])\n",
    "    predictor = predictor.flatten()\n",
    "\n",
    "    baseline_mean = np.mean(predictor)\n",
    "    predictor = predictor - baseline_mean\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        S_a = predictor[col]\n",
    "        valid_days = np.where(cumulative <= -(a + b * S_a))[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    return np.array(doy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4a89468-992e-465d-91cb-a818b7270c97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SIAMN\n",
    "def SIAMN_model(par, predictor, data):\n",
    "    if len(par) != 5:\n",
    "        raise ValueError(\"Expected 5 parameters: T_b, a, F_crit\")\n",
    "\n",
    "    T_b, P_base, a, a1, b1 = par\n",
    "    Tmini = data['Tmini']  # (days, sites)\n",
    "    Li = data['Li']            # shape: (days, sites)\n",
    "\n",
    "    # Chilling response function (sigmoid form, negative values only)\n",
    "    Rf = -(1 - 1 / (1 + np.exp(-a * (Tmini - T_b)))) * (Li / P_base)\n",
    "    Rf[Rf > 0] = 0  # Only chilling (negative) values allowed\n",
    "\n",
    "    # Determine t0: first day after day 173 where Tmean < T_b\n",
    "    t0 = []\n",
    "    days = np.arange(1, Tmini.shape[0] + 1)\n",
    "\n",
    "    for c in range(Tmini.shape[1]):\n",
    "        below_base = np.where(Tmini[:, c] < T_b)[0]\n",
    "        after_173 = below_base[below_base > 173]\n",
    "\n",
    "        if len(after_173) == 0:\n",
    "            t0.append(Tmini.shape[0])  # No valid chilling period\n",
    "        else:\n",
    "            t0.append(after_173[0])\n",
    "\n",
    "    # Zero out chilling accumulation before t0\n",
    "    for c, t0_day in enumerate(t0):\n",
    "        Rf[:t0_day+1, c] = 0  # Include t0\n",
    "\n",
    "    # Determine DOY: first day when cumulative chilling <= F_crit\n",
    "    predictor = predictor.flatten()\n",
    "\n",
    "    baseline_mean = np.mean(predictor)\n",
    "    predictor = predictor - baseline_mean\n",
    "    doy = []\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        S_a = predictor[col]\n",
    "        valid_days = np.where(cumulative <= -(a1 + b1 * S_a))[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    return np.array(doy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "098be08e-e2a7-4b11-8797-53436ab1ee06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SIAMNP\n",
    "def SIAMNP_model(par, predictor, data):\n",
    "    if len(par) != 7:\n",
    "        raise ValueError(\"Expected 3 parameters: T_b, a, F_crit\")\n",
    "\n",
    "    T_b, P_base, a, a1, b1, c, p_opt = par\n",
    "    Tmini = data['Tmini']  # (days, sites)\n",
    "    Li = data['Li']            # shape: (days, sites)\n",
    "    Pmean = data['Pmean']  # (days, sites)\n",
    "\n",
    "    # p_opt = 1.25\n",
    "    P_mod = 1 - np.exp(-((Pmean - p_opt) ** 2) / (2 * c ** 2))  # shape: (days, sites)\n",
    "\n",
    "    # Chilling response function (sigmoid form, negative values only)\n",
    "    Rf = -(1 - 1 / (1 + np.exp(-a * (Tmini - T_b)))) * (Li / P_base)\n",
    "    Rf[Rf > 0] = 0  # Only chilling (negative) values allowed\n",
    "\n",
    "    # Determine t0: first day after day 173 where Tmean < T_b\n",
    "    t0 = []\n",
    "    days = np.arange(1, Tmini.shape[0] + 1)\n",
    "\n",
    "    for c in range(Tmini.shape[1]):\n",
    "        below_base = np.where(Tmini[:, c] < T_b)[0]\n",
    "        after_173 = below_base[below_base > 173]\n",
    "\n",
    "        if len(after_173) == 0:\n",
    "            t0.append(Tmini.shape[0])  # No valid chilling period\n",
    "        else:\n",
    "            t0.append(after_173[0])\n",
    "\n",
    "    # Zero out chilling accumulation before t0\n",
    "    for c, t0_day in enumerate(t0):\n",
    "        Rf[:t0_day+1, c] = 0  # Include t0\n",
    "\n",
    "    # Determine DOY: first day when cumulative chilling <= F_crit\n",
    "    predictor = predictor.flatten()\n",
    "    P_mod = P_mod.flatten()\n",
    "\n",
    "    baseline_mean = np.mean(predictor)\n",
    "    predictor = predictor - baseline_mean\n",
    "    \n",
    "    doy = []\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        S_a = predictor[col]\n",
    "        valid_days = np.where(cumulative <= -(a1 + b1 * S_a) * (1 - P_mod[col]))[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    return np.array(doy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18ddc1-1184-44d1-aa5d-427c07274a36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Convert envir data\n",
    "def convert_envir_data(envir_data):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Extract relevant columns\n",
    "    min_cols = [col for col in envir_data.columns if 'daily_min_t' in col]\n",
    "\n",
    "    id_vars = ['latitude', 'longitude']\n",
    "\n",
    "    # Melt temperature datasets\n",
    "    min_df = envir_data[id_vars + min_cols].melt(\n",
    "        id_vars=id_vars, value_name='temp_min', var_name='date_col')\n",
    "\n",
    "    # Extract date from column name\n",
    "    # for df in [min_df, mean_df, max_df]:\n",
    "    for df in [min_df]:\n",
    "        df['date'] = df['date_col'].str.extract(r'(\\d{4}_\\d{2}_\\d{2})')\n",
    "\n",
    "    merged_df = min_df\n",
    "    \n",
    "    # Convert to datetime, extract year and DOY\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'], format='%Y_%m_%d')\n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    merged_df['doy'] = merged_df['date'].dt.dayofyear\n",
    "\n",
    "    # Convert from Kelvin to Celsius\n",
    "    merged_df['temp_min'] = merged_df['temp_min'] - 273.5\n",
    "\n",
    "    # Pivot to get one column per year/temp type\n",
    "    pivot_df = merged_df.pivot_table(\n",
    "        index=['latitude', 'longitude', 'doy'],\n",
    "        columns='year',\n",
    "        values=['temp_min']\n",
    "    ).reset_index()\n",
    "\n",
    "    # Flatten column MultiIndex and rename properly\n",
    "    pivot_df.columns = ['latitude', 'longitude', 'doy'] + [\n",
    "        col.replace('temp_min', 'daily_min_t')\n",
    "        for col in [f\"{var}_{year}\" for var, year in pivot_df.columns[3:]]\n",
    "    ]\n",
    "\n",
    "    # Generate full index of all DOYs per pixel\n",
    "    all_doys = pd.DataFrame({'doy': range(1, 367)})\n",
    "    pixels = pivot_df[['latitude', 'longitude']].drop_duplicates()\n",
    "    full_index = pixels.merge(all_doys, how='cross')\n",
    "\n",
    "    # Merge with full pivoted data\n",
    "    merged_full = pd.merge(full_index, pivot_df, on=['latitude', 'longitude', 'doy'], how='left')\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in merged_full.columns:\n",
    "        if 'daily_min_t' in col or 'daily_mean_t' in col or 'daily_max_t' in col:\n",
    "            merged_full[col] = merged_full[col].fillna(999)\n",
    "\n",
    "    return merged_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f90570b1-e546-48dd-8b78-6b3d3fc30c48",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Read AVHRR data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import dual_annealing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import re\n",
    "def filter_satellite_data():\n",
    "    # Load static datasets\n",
    "    veg_class = pd.read_csv(r'..\\..\\Shared Code\\data\\tables\\veg_class\\veg_class.csv')\n",
    "    df_satellite = pd.read_csv(r'..\\..\\Satellite Code\\data\\tables\\phenology_climate\\avhrr.csv')\n",
    "    \n",
    "    # Merge satellite and vegetation class data\n",
    "    df = pd.merge(df_satellite, veg_class, on=['latitude', 'longitude'], how='inner')\n",
    "    \n",
    "    df = df[df['veg_class'].isin([11, 12, 13, 14])]\\\n",
    "            # .sample(frac=1, random_state=22)\\\n",
    "            # .reset_index(drop=True)\\\n",
    "            # .iloc[60000:]\n",
    "    \n",
    "    print(df.shape)\n",
    "    # print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e795fad-3f7e-4806-b91c-ad14affc7716",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process and merge temperature data from 1982 to 2016\n",
    "def process_daily_min_t_data(df):\n",
    "    for year in range(1982, 2017):\n",
    "        path_min = f'../data/tables/daily_t_1982-2016/daily_min_t/daily_min_t_{year}.csv'\n",
    "    \n",
    "        if os.path.exists(path_min):# and os.path.exists(path_mean) and os.path.exists(path_max):\n",
    "            df_min = pd.read_csv(path_min).drop(columns=[\"veg_class\"])\n",
    "            df_min = aggregate_pixel(df_min)\n",
    "            # print(df_min)\n",
    "            def fix_column_names(df, year, prefix):\n",
    "                new_columns = {}\n",
    "                for col in df.columns:\n",
    "                    new_col = col\n",
    "                    if '_1982_' in col:\n",
    "                        new_col = new_col.replace('1982', str(year))\n",
    "                    # Ensure correct prefix\n",
    "                    if f'daily_{prefix}_t_' not in new_col:\n",
    "                        for p in ['min', 'mean', 'max']:\n",
    "                            new_col = new_col.replace(f'daily_{p}_t_', f'daily_{prefix}_t_')\n",
    "                    new_columns[col] = new_col\n",
    "                return df.rename(columns=new_columns)\n",
    "            \n",
    "            # Apply renaming\n",
    "            df_min = fix_column_names(df_min, year, prefix=\"min\")\n",
    "            df_year = df_min\n",
    "            \n",
    "            df = df.merge(df_year, on=['latitude', 'longitude'], how='inner')\n",
    "            # print(df.shape)\n",
    "        else:\n",
    "            print(f\"Missing data for year {year}. Skipping.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904de64f-de81-4477-b78b-f9398c20511a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract phenology data\n",
    "def extract_phenology_data(df):\n",
    "    eos_sos_data = df.loc[:, df.columns.str.contains('sos_|eos_|latitude|longitude')]\n",
    "    \n",
    "    # Extract environmental data\n",
    "    cols = [col for col in df.columns if any(f't_{year}' in col for year in map(str, range(1982, 2017)))] + ['latitude', 'longitude']\n",
    "\n",
    "    # Drop seasonal/annual summaries if they exist\n",
    "    seasonal_cols = ['spring', 'summer', 'autumn', 'annual']\n",
    "    for year in range(1982, 2017):\n",
    "        for season in seasonal_cols:\n",
    "            col_name = f'{season}_t_{year}'\n",
    "            if col_name in cols:\n",
    "                cols.remove(col_name)\n",
    "    \n",
    "    envir_data = df[cols]\n",
    "\n",
    "    # Convert environmental data\n",
    "    envir_data = convert_envir_data(envir_data)\n",
    "    # De-fragment DataFrame to improve performance\n",
    "    envir_data = envir_data.copy()\n",
    "    # Add day length\n",
    "    envir_data['Li'] = envir_data.apply(lambda row: day_length(row['doy'], row['latitude']), axis=1)\n",
    "    \n",
    "    prcp_data = df.loc[:, df.columns.str.contains('annual_p|latitude|longitude')]\n",
    "    return eos_sos_data, envir_data, prcp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122bea6-afaa-40c5-8df7-5dfad1ab41bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3beb2-1b35-404d-93d1-e014d1a92c42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train, validate, and plot model performance by pixel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import dual_annealing\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.optimize import minimize\n",
    "def get_model_list():\n",
    "    model_list = {\n",
    "        'CDD': {'fun': CDD_model, 'init': [20, -20], 'lower': [10, -5000], 'upper': [25, 0]},\n",
    "        'DM': {'fun': DM_model, 'init': [20, 12, -20], 'lower': [10, 10, -3300], 'upper': [25, 16, 0]},\n",
    "        'SIAM': {'fun': SIAM_model, 'init': [20, 12, 20, 0], 'lower': [10, 10, 0, -2], 'upper': [25, 16, 2200, 2]},\n",
    "        'SIAMN': {'fun': SIAMN_model, 'init': [20, 12, 0.5, 20, 0], 'lower': [15, 10, 0.01, 1, -2], 'upper': [30, 16, 2, 235, 2]},  \n",
    "        'SIAMNP': {'fun': SIAMNP_model, 'init': [20, 12, 0.5, 20, 0, 2, 1], 'lower': [15, 10, 0.01, 1, -1, 1, 0.05], 'upper': [30, 16, 2, 295, 1, 10, 3]},  \n",
    "    }\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45fb71-e0a1-4edb-8a39-8459e1f5badb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate EOS models\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "def evaluate_models(eos_sos_data, envir_data, prcp_data, model_list):\n",
    "    start_time = time.time()\n",
    "    maxiteration = 10\n",
    "\n",
    "    # Extract available years from eos_sos_data\n",
    "    all_eos_cols = [col for col in eos_sos_data.columns if col.startswith('eos_')]\n",
    "    available_years = sorted([int(col.split('_')[1]) for col in all_eos_cols])\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    pixel_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for pixel_idx, site in eos_sos_data.iterrows():\n",
    "        target_lat = site['latitude']\n",
    "        target_lon = site['longitude']\n",
    "    \n",
    "        envir_sub = envir_data[\n",
    "            (envir_data[\"latitude\"] == target_lat) & \n",
    "            (envir_data[\"longitude\"] == target_lon)\n",
    "        ].dropna()\n",
    "        prcp_sub = prcp_data[\n",
    "            (prcp_data[\"latitude\"] == target_lat) & \n",
    "            (prcp_data[\"longitude\"] == target_lon)\n",
    "        ].dropna()\n",
    "        \n",
    "        # Collect metrics across folds\n",
    "        pixel_test_rmse_all_folds = {model: [] for model in model_list}\n",
    "        pixel_test_r2_all_folds = {model: [] for model in model_list}\n",
    "        pixel_test_slope_all_folds = {model: [] for model in model_list}\n",
    "        pixel_test_p_all_folds = {model: [] for model in model_list}\n",
    "        \n",
    "        #  Collect parameter vectors across folds\n",
    "        pixel_params_all_folds = {model: [] for model in model_list}\n",
    "    \n",
    "        for train_idx, test_idx in kf.split(available_years):\n",
    "            train_years = [available_years[i] for i in train_idx]\n",
    "            test_years = [available_years[i] for i in test_idx]\n",
    "    \n",
    "            eos_train_cols = [f'eos_{year}' for year in train_years]\n",
    "            eos_test_cols = [f'eos_{year}' for year in test_years]\n",
    "            min_t_train_cols = [f'daily_min_t_{year}' for year in train_years]\n",
    "            min_t_test_cols = [f'daily_min_t_{year}' for year in test_years]\n",
    "            mean_p_train_cols = [f'annual_p_{year}' for year in train_years]\n",
    "            mean_p_test_cols = [f'annual_p_{year}' for year in test_years]\n",
    "            sos_train_cols = [f'sos_{year}' for year in train_years]\n",
    "            sos_test_cols = [f'sos_{year}' for year in test_years]\n",
    "            Li_train = np.tile(envir_sub['Li'].values[:, None], (1, len(train_years)))\n",
    "            Li_test = np.tile(envir_sub['Li'].values[:, None], (1, len(test_years)))\n",
    "    \n",
    "            for model_name, model_info in model_list.items():\n",
    "                observed_DOY_train = site[eos_train_cols].astype(float).to_numpy()\n",
    "    \n",
    "                if model_name in ['NCDD2', 'PC_NCDD2']:\n",
    "                    data_list_train = {\n",
    "                        'Tmini': envir_sub[min_t_train_cols].to_numpy(),\n",
    "                        'Li': Li_train,\n",
    "                        'transition_dates': observed_DOY_train\n",
    "                    }\n",
    "                elif model_name in ['SIAMNP', 'SIAMP']:\n",
    "                    data_list_train = {\n",
    "                        'Tmini': envir_sub[min_t_train_cols].to_numpy(),\n",
    "                        'Pmean': prcp_sub[mean_p_train_cols].to_numpy(),\n",
    "                        'Li': Li_train,\n",
    "                        'transition_dates': observed_DOY_train\n",
    "                    }\n",
    "                else:\n",
    "                    data_list_train = {\n",
    "                        'Tmini': envir_sub[min_t_train_cols].to_numpy(),\n",
    "                        'Li': Li_train,\n",
    "                        'transition_dates': observed_DOY_train\n",
    "                    }\n",
    "    \n",
    "                if model_name in ['SIAM', 'SIAMN', 'SIAMNP', 'SIAMP']:\n",
    "                    predictor1 = site[sos_train_cols].astype(float).to_numpy()\n",
    "                    \n",
    "                def cost_function(par):\n",
    "                    if not np.all(np.isfinite(par)):\n",
    "                        return 1e10\n",
    "                    try:\n",
    "                        if model_name in ['SIAM', 'SIAMN', 'SIAMNP', 'SIAMP']:\n",
    "                            predicted = model_info['fun'](par, predictor1, data_list_train)\n",
    "                        else:\n",
    "                            predicted = model_info['fun'](par, data_list_train)\n",
    "                    except:\n",
    "                        return 1e10\n",
    "    \n",
    "                    if predicted is None or not np.all(np.isfinite(predicted)):\n",
    "                        return 1e10\n",
    "                    return np.sqrt(np.mean((predicted - observed_DOY_train) ** 2))\n",
    "    \n",
    "                bounds = list(zip(model_info['lower'], model_info['upper']))\n",
    "                result = dual_annealing(cost_function, bounds=bounds, maxiter=maxiteration, seed=42)\n",
    "                \n",
    "                # NEW: store params for this fold and model\n",
    "                if result is not None and np.all(np.isfinite(result.x)):\n",
    "                    pixel_params_all_folds[model_name].append(result.x.copy())\n",
    "                else:\n",
    "                    # optionally append NaNs to preserve fold count\n",
    "                    pixel_params_all_folds[model_name].append(np.full(len(model_info['lower']), np.nan))\n",
    "                \n",
    "                observed_DOY_test = site[eos_test_cols].astype(float).to_numpy()\n",
    "    \n",
    "                if model_name in ['NCDD2', 'PC_NCDD2']:\n",
    "                    data_list_test = {\n",
    "                        'Tmini': envir_sub[min_t_test_cols].to_numpy(),\n",
    "                        'Li': Li_test,\n",
    "                        'transition_dates': observed_DOY_test\n",
    "                    }\n",
    "                elif model_name in ['SIAMNP', 'SIAMP']:\n",
    "                    data_list_test = {\n",
    "                        'Tmini': envir_sub[min_t_test_cols].to_numpy(),\n",
    "                        'Pmean': prcp_sub[mean_p_test_cols].to_numpy(),\n",
    "                        'Li': Li_test,\n",
    "                        'transition_dates': observed_DOY_test\n",
    "                    }\n",
    "                else:\n",
    "                    data_list_test = {\n",
    "                        'Tmini': envir_sub[min_t_test_cols].to_numpy(),\n",
    "                        'Li': Li_test,\n",
    "                        'transition_dates': observed_DOY_test\n",
    "                    }\n",
    "    \n",
    "                if model_name in ['SIAM', 'SIAMN', 'SIAMNP', 'SIAMP']:\n",
    "                    predictor1 = site[sos_test_cols].astype(float).to_numpy()\n",
    "                    predicted_test = model_info['fun'](result.x, predictor1, data_list_test)\n",
    "                else:\n",
    "                    predicted_test = model_info['fun'](result.x, data_list_test)\n",
    "    \n",
    "                valid_mask = (~np.isnan(observed_DOY_test)) & (~np.isnan(predicted_test))\n",
    "    \n",
    "                if np.any(valid_mask):\n",
    "                    obs = observed_DOY_test[valid_mask]\n",
    "                    pred = predicted_test[valid_mask]\n",
    "                    residuals = pred - obs\n",
    "    \n",
    "                    if np.std(pred) == 0 or np.std(obs) == 0:\n",
    "                        r2 = np.nan\n",
    "                        slope = np.nan\n",
    "                        p_val = np.nan\n",
    "                    else:\n",
    "                        slope, intercept, r_value, p_val, std_err = linregress(pred, obs)\n",
    "                        r2 = r_value**2\n",
    "                    rmse = np.sqrt(np.mean(residuals ** 2))\n",
    "                else:\n",
    "                    rmse, r2, slope, p_val = np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "                pixel_test_rmse_all_folds[model_name].append(rmse)\n",
    "                pixel_test_r2_all_folds[model_name].append(r2)\n",
    "                pixel_test_slope_all_folds[model_name].append(slope)\n",
    "                pixel_test_p_all_folds[model_name].append(p_val)\n",
    "    \n",
    "        # Averages across folds for metrics\n",
    "        avg_rmse = {k: np.nanmean(v) for k, v in pixel_test_rmse_all_folds.items()}\n",
    "        avg_r2 = {k: np.nanmean(v) for k, v in pixel_test_r2_all_folds.items()}\n",
    "        avg_slope = {k: np.nanmean(v) for k, v in pixel_test_slope_all_folds.items()}\n",
    "        avg_p = {k: np.nanmean(v) for k, v in pixel_test_p_all_folds.items()}\n",
    "    \n",
    "        # Average (and std) parameters across folds for each model for this pixel\n",
    "        # Convert lists of vectors to arrays and average along axis 0\n",
    "        avg_params = {}\n",
    "        std_params = {}\n",
    "        for model_name, plist in pixel_params_all_folds.items():\n",
    "            # Keep only arrays of the right shape; handle all-NaN case\n",
    "            if len(plist) == 0:\n",
    "                avg_params[model_name] = None\n",
    "                std_params[model_name] = None\n",
    "                continue\n",
    "            param_mat = np.vstack(plist)  # shape: (n_folds, n_params)\n",
    "            avg_params[model_name] = np.nanmean(param_mat, axis=0).tolist()\n",
    "            std_params[model_name] = np.nanstd(param_mat, axis=0).tolist()\n",
    "    \n",
    "        pixel_results.append({\n",
    "            'latitude': target_lat,\n",
    "            'longitude': target_lon,\n",
    "            'test_rmse': avg_rmse,\n",
    "            'test_r2': avg_r2,\n",
    "            'test_slope': avg_slope,\n",
    "            'test_p': avg_p,\n",
    "            # Save mean and std params per model for this pixel\n",
    "            'mean_params': avg_params,\n",
    "            'std_params': std_params\n",
    "        })\n",
    "    \n",
    "    print(\"Calibration done!\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    return pixel_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8761a9e4-4c45-4414-a07a-c5fb6c6c3edb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Aggregate pixels\n",
    "def aggregate_pixel(df):\n",
    "    df[\"latitude\"] = snap_to_array(df[\"latitude\"].values, latitude)\n",
    "    df[\"longitude\"] = snap_to_array(df[\"longitude\"].values, longitude)\n",
    "    df = df.groupby([\"latitude\", \"longitude\"], as_index=False).mean()\n",
    "    df[\"latitude\"] = df[\"latitude\"].round(4)\n",
    "    df[\"longitude\"] = df[\"longitude\"].round(4)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24da4e34-11e7-402e-9d83-cf020b1fff39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128264, 458)\n"
     ]
    }
   ],
   "source": [
    "df = filter_satellite_data()\n",
    "df = aggregate_pixel(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12d02d40-cdc2-4786-95dd-d216b7d95964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1600:] ## Change to [0:1600] or [1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2ff1089-f671-4dc0-8dbd-bdd282d32cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_daily_min_t_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1b519aa-0722-4534-a508-6d1e482b97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = get_model_list()\n",
    "eos_sos_data, envir_data, prcp_data = extract_phenology_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeb9e2-9c2b-4f46-a043-fb4283de8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_models(eos_sos_data, envir_data, prcp_data, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b338ca8-d50d-41b7-ad08-0524f9842eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(evaluation_results, \"..\\\\data\\\\tables\\\\params\\\\1982-2016\\\\evaluation_results_1600_3200.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478dc7f-6725-4971-8862-04d1223be5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
