{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "805b37c1-5680-4b83-88fe-2a2855d4b13b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Daylength\n",
    "import numpy as np\n",
    "\n",
    "def day_length(day_of_year, latitude):\n",
    "    # Ensure arrays are numpy arrays for vectorized operations\n",
    "    day_of_year = np.array(day_of_year)\n",
    "    latitude_rad = np.deg2rad(latitude)\n",
    "    \n",
    "    P = np.arcsin(0.39795 * np.cos(0.2163108 + 2 * np.arctan(0.9671396 * np.tan(0.00860 * (day_of_year - 186)))))\n",
    "    \n",
    "    numerator = np.sin(np.deg2rad(0.8333)) + np.sin(latitude_rad) * np.sin(P)\n",
    "    denominator = np.cos(latitude_rad) * np.cos(P)\n",
    "    acos_arg = numerator / denominator\n",
    "\n",
    "    # Clip the value to avoid NaN due to domain errors in arccos\n",
    "    acos_arg = np.clip(acos_arg, -1.0, 1.0)\n",
    "\n",
    "    day_light_hours = 24 - (24 / np.pi) * np.arccos(acos_arg)\n",
    "    return day_light_hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df1ef57e-8db5-475b-90cb-deab4d681345",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## CDD\n",
    "def CDD_model(par, data):\n",
    "    if len(par) != 2:\n",
    "        raise ValueError(\"model parameter(s) out of range (too many, too few)\")\n",
    "\n",
    "    T_base, F_crit = par\n",
    "    Tmini = data['Tmini']  # shape: (366, num_sites)\n",
    "    # Step 1: Calculate chilling rate (Rf)\n",
    "    Rf = Tmini - T_base\n",
    "    Rf[Rf > 0] = 0\n",
    "\n",
    "    # Step 2: Determine start day (t0) for chilling accumulation after day 200\n",
    "    t0 = []\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        temp_series = Tmini[:, col]\n",
    "        below_base = np.where(temp_series < T_base)[0]\n",
    "        later_days = below_base[below_base > 173]\n",
    "        if len(later_days) == 0:\n",
    "            t0.append(174)  # Default to day 201 if none found\n",
    "        else:\n",
    "            t0.append(later_days[0])\n",
    "\n",
    "    # Step 3: Nullify values before t0\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        Rf[:t0[col], col] = 0\n",
    "\n",
    "    # Step 4: Compute doy when cumulative Rf exceeds F_crit\n",
    "    doy = []\n",
    "    # print(Rf.shape[1])\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        valid_days = np.where(cumulative <= F_crit)[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    # print(doy) \n",
    "    return np.array(doy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ca4bb6-839f-4e92-acae-8e36738c2efa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## DM\n",
    "def DM_model(par, data):\n",
    "    if len(par) != 3:\n",
    "        raise ValueError(\"model parameter(s) out of range (too many, too few)\")\n",
    "\n",
    "    T_base, P_base, F_crit = par\n",
    "    Tmini = data['Tmini']      # shape: (days, sites)\n",
    "    Li = data['Li']            # shape: (days, sites)\n",
    "\n",
    "    # Rate function: only negative values contribute\n",
    "    Rf = (Tmini - T_base)# * (Li / P_base)\n",
    "    Rf[Rf > 0] = 0\n",
    "\n",
    "    # Step 2: Determine start day (t0) for chilling accumulation after day 200\n",
    "    t0 = []\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        temp_series = Tmini[:, col]\n",
    "        below_base = np.where(temp_series < T_base)[0]\n",
    "        later_days = below_base[below_base > 173]\n",
    "        if len(later_days) == 0:\n",
    "            t0.append(174)  # Default to day 201 if none found\n",
    "        else:\n",
    "            t0.append(later_days[0])\n",
    "\n",
    "    # Step 3: Nullify values before t0\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        Rf[:t0[col], col] = 0\n",
    "\n",
    "    # Step 4: Compute doy when cumulative Rf exceeds F_crit\n",
    "    doy = []\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        valid_days = np.where(cumulative <= F_crit)[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    return np.array(doy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d92456d1-658b-4248-a333-77e34504f110",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SIAM\n",
    "def SIAM_model(par, predictor, data):\n",
    "    if len(par) != 4:\n",
    "        raise ValueError(\"model parameter(s) out of range (too many, too few)\")\n",
    "\n",
    "    T_base, P_base, a, b = par\n",
    "    Tmini = data[\"Tmini\"]\n",
    "    Li = data[\"Li\"]\n",
    "\n",
    "    Rf = (Tmini - T_base) * (Li / P_base)\n",
    "    Rf[Rf > 0] = 0\n",
    "\n",
    "    # Step 2: Determine start day (t0) for chilling accumulation after day 200\n",
    "    t0 = []\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        temp_series = Tmini[:, col]\n",
    "        below_base = np.where(temp_series < T_base)[0]\n",
    "        later_days = below_base[below_base > 173]\n",
    "        if len(later_days) == 0:\n",
    "            t0.append(174)  # Default to day 201 if none found\n",
    "        else:\n",
    "            t0.append(later_days[0])\n",
    "\n",
    "    # Step 3: Nullify values before t0\n",
    "    for col in range(Tmini.shape[1]):\n",
    "        Rf[:t0[col], col] = 0\n",
    "\n",
    "    # Step 4: Compute doy when cumulative Rf exceeds F_crit\n",
    "    doy = []\n",
    "    predictor = predictor.flatten()\n",
    "    baseline_mean = np.nanmean(predictor)\n",
    "    predictor = predictor - baseline_mean\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        S_a = predictor[col]\n",
    "        valid_days = np.where(cumulative <= -(a + b * S_a))[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    # print(doy) \n",
    "    return np.array(doy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "778e3929-3c35-4dd8-9901-e007c4e60c74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SIAMN\n",
    "def SIAMN_model(par, predictor, data):\n",
    "    if len(par) != 5:\n",
    "        raise ValueError(\"Expected 3 parameters: T_b, a, F_crit\")\n",
    "\n",
    "    T_b, P_base, a, a1, b1 = par\n",
    "    Tmini = data['Tmini']  # (days, sites)\n",
    "    Li = data['Li']            # shape: (days, sites)\n",
    "\n",
    "    # Chilling response function (sigmoid form, negative values only)\n",
    "    Rf = -(1 - 1 / (1 + np.exp(-a * (Tmini - T_b)))) * (Li / P_base)\n",
    "    Rf[Rf > 0] = 0  # Only chilling (negative) values allowed\n",
    "    # Determine t0: first day after day 173 where Tmean < T_b\n",
    "    t0 = []\n",
    "    days = np.arange(1, Tmini.shape[0] + 1)\n",
    "\n",
    "    for c in range(Tmini.shape[1]):\n",
    "        below_base = np.where(Tmini[:, c] < T_b)[0]\n",
    "        after_173 = below_base[below_base > 173]\n",
    "\n",
    "        if len(after_173) == 0:\n",
    "            t0.append(Tmini.shape[0])  # No valid chilling period\n",
    "        else:\n",
    "            t0.append(after_173[0])\n",
    "\n",
    "    # Zero out chilling accumulation before t0\n",
    "    for c, t0_day in enumerate(t0):\n",
    "        Rf[:t0_day+1, c] = 0  # Include t0\n",
    "\n",
    "    # Determine DOY: first day when cumulative chilling <= F_crit\n",
    "    predictor = predictor.flatten()\n",
    "    baseline_mean = np.nanmean(predictor)\n",
    "    predictor = predictor - baseline_mean\n",
    "    doy = []\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        S_a = predictor[col]\n",
    "        valid_days = np.where(cumulative <= -(a1 + b1 * S_a))[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    # print(doy) \n",
    "    return np.array(doy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8f51103-55b2-45ca-82f1-696418a25f7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SIAMNP\n",
    "def SIAMNP_model(par, predictor, data):\n",
    "    if len(par) != 7:\n",
    "        raise ValueError(\"Expected 3 parameters: T_b, a, F_crit\")\n",
    "\n",
    "    T_b, P_base, a, a1, b1, c, p_opt = par\n",
    "    Tmini = data['Tmini']  # (days, sites)\n",
    "    Li = data['Li']            # shape: (days, sites)\n",
    "    Pmean = data['Pmean']  # (days, sites)\n",
    "\n",
    "    # p_opt = 1.25\n",
    "    P_mod = 1 - np.exp(-((Pmean - p_opt) ** 2) / (2 * c ** 2))  # shape: (days, sites)\n",
    "\n",
    "    # Chilling response function (sigmoid form, negative values only)\n",
    "    Rf = -(1 - 1 / (1 + np.exp(-a * (Tmini - T_b)))) * (Li / P_base)\n",
    "    Rf[Rf > 0] = 0  # Only chilling (negative) values allowed\n",
    "    # Determine t0: first day after day 173 where Tmean < T_b\n",
    "    t0 = []\n",
    "    days = np.arange(1, Tmini.shape[0] + 1)\n",
    "\n",
    "    for c in range(Tmini.shape[1]):\n",
    "        below_base = np.where(Tmini[:, c] < T_b)[0]\n",
    "        after_173 = below_base[below_base > 173]\n",
    "\n",
    "        if len(after_173) == 0:\n",
    "            t0.append(Tmini.shape[0])  # No valid chilling period\n",
    "        else:\n",
    "            t0.append(after_173[0])\n",
    "\n",
    "    # Zero out chilling accumulation before t0\n",
    "    for c, t0_day in enumerate(t0):\n",
    "        Rf[:t0_day+1, c] = 0  # Include t0\n",
    "\n",
    "    # Determine DOY: first day when cumulative chilling <= F_crit\n",
    "    predictor = predictor.flatten()\n",
    "    P_mod = P_mod.flatten()\n",
    "\n",
    "    baseline_mean = np.mean(predictor)\n",
    "    predictor = predictor - baseline_mean\n",
    "    \n",
    "    doy = []\n",
    "    for col in range(Rf.shape[1]):\n",
    "        cumulative = np.cumsum(Rf[:, col])\n",
    "        S_a = predictor[col]\n",
    "        valid_days = np.where(cumulative <= -(a1 + b1 * S_a) * (1 - P_mod[col]))[0]\n",
    "        doy_val = valid_days[0] + 1 if len(valid_days) > 0 else np.nan\n",
    "        doy.append(doy_val)\n",
    "    return np.array(doy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdf10f-b7bd-4959-a030-063a5e8df818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb214093-7ce4-4f1e-b1d2-4e132bde717f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "def read_data():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.optimize import dual_annealing\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # ---------------------------------\n",
    "    # 1. Read and Prepare Data\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    eos_sos_data = pd.read_csv('../data/tables/spruce/sos_eos_2016_2023_ORNL.csv')\n",
    "\n",
    "    # Drop columns matching eos_2014, eos_2015, sos_2014, sos_2015\n",
    "    eos_sos_data = eos_sos_data.drop(\n",
    "        columns=[col for col in eos_sos_data.columns if any(x in col for x in ['eos_2014', 'eos_2015', 'sos_2014', 'sos_2015'])])\n",
    " \n",
    "    exclude_plots = [4, 10, 11, 16, 19]\n",
    "    eos_sos_data = eos_sos_data[~eos_sos_data['plot'].isin(exclude_plots)].dropna()\n",
    "    # print(eos_sos_data)\n",
    "    \n",
    "    envir_data = pd.read_csv('../data/tables/spruce/envir_data.csv')\n",
    "    # Drop columns matching certain years and variables\n",
    "    drop_patterns = ['TA_2_0_1_min_2014', 'TA_2_0_1_min_2015', 'TA_2_0_1_min_2024', 'TA_2_0_1_min_2025',\n",
    "                     'TA_2_0_1_max_2014', 'TA_2_0_1_max_2015', 'TA_2_0_1_max_2024', 'TA_2_0_1_max_2025']\n",
    "  \n",
    "    cols_to_drop = [col for col in envir_data.columns if any(p in col for p in drop_patterns)]\n",
    "    envir_data = envir_data.drop(columns=cols_to_drop)\n",
    "    envir_data.columns = (envir_data.columns\n",
    "                  .str.replace(r'^TA_2_0_1_min_', 'daily_min_t_', regex=True)\n",
    "                  .str.replace(r'^TA_2_0_1_max_', 'daily_max_t_', regex=True))\n",
    "    # print(envir_data)\n",
    "    \n",
    "    # Add day length (Li) column\n",
    "    envir_data['Li'] = envir_data.apply(lambda row: day_length(row['doy'], 47.51), axis=1)\n",
    "    eos_sos_data['site_index'] = eos_sos_data.index\n",
    "    # print(eos_sos_data)\n",
    "    site_index_by_plot = eos_sos_data.groupby('plot')['site_index'].apply(list)\n",
    "    rows = []\n",
    "    for _, row in envir_data.iterrows():\n",
    "        plot = row['plot']\n",
    "        if plot in site_index_by_plot:\n",
    "            for site_index in site_index_by_plot[plot]:\n",
    "                new_row = row.copy()\n",
    "                new_row['site_index'] = site_index\n",
    "                rows.append(new_row)\n",
    "    repeated_envir_data = pd.DataFrame(rows).sort_values(by=['site_index', 'doy']).reset_index(drop=True)\n",
    "    # print(repeated_envir_data)\n",
    "    envir_data = repeated_envir_data\n",
    "    # print(envir_data)\n",
    "    \n",
    "    prcp_data = pd.read_csv('../data/tables/spruce/spruce_prcp.csv')#.to_numpy()\n",
    "    exclude_mask = (\n",
    "        prcp_data.columns.str.contains(r'_(?:2015|2024)$', regex=True) |\n",
    "        prcp_data.columns.isin(['latitude', 'longitude'])\n",
    "    )\n",
    "\n",
    "    prcp_data = prcp_data.loc[:, ~exclude_mask]\n",
    "    # print(prcp_data)\n",
    "    repeats = len(eos_sos_data)\n",
    "    repeated_df = pd.concat([prcp_data] * repeats, ignore_index=True)\n",
    "    site_index = pd.Series(range(repeats)).repeat(len(prcp_data)).reset_index(drop=True)\n",
    "    repeated_df['site_index'] = site_index\n",
    "    prcp_data = repeated_df\n",
    "    # print(prcp_data)\n",
    "    return eos_sos_data, envir_data, prcp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1ba57-eddc-467b-99cb-ebd9a9f7d1ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Split data\n",
    "def split_data(eos_sos_data_train, eos_sos_data_test, envir_data_train, envir_data_test, prcp_data_train, prcp_data_test):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from scipy.optimize import dual_annealing\n",
    "    from scipy.stats import pearsonr\n",
    "    from scipy.optimize import differential_evolution\n",
    "    from scipy.optimize import minimize\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    ## Sort columns\n",
    "    def sort_pixel_year_columns(columns):\n",
    "        def extract_pixel_year(col):\n",
    "            # Try to extract pixel and year\n",
    "            match = re.search(r'pixel_(\\d+).*_(\\d{4})', col)\n",
    "            if match:\n",
    "                pixel = int(match.group(1))\n",
    "                year = int(match.group(2))\n",
    "                return (pixel, year)\n",
    "            elif col == \"var_new\":\n",
    "                return (-1, -1)  # Keep var_new at the front\n",
    "            else:\n",
    "                return (float('inf'), float('inf'))  # Other columns go last\n",
    "    \n",
    "        return sorted(columns, key=extract_pixel_year)\n",
    "        \n",
    "    def merge_eos_sos_data(df):\n",
    "        df_flat = df.copy()\n",
    "        df_flat.set_index(\"site_index\", inplace=True)\n",
    "        new_columns = [f\"pixel_{idx}_{col}\" for idx in df_flat.index for col in df_flat.columns]\n",
    "        df_single_row = pd.DataFrame([df_flat.values.flatten()], columns=new_columns)\n",
    "        return df_single_row\n",
    "    # print(eos_sos_data_train)\n",
    "    eos_sos_train = merge_eos_sos_data(eos_sos_data_train)\n",
    "    eos_sos_test = merge_eos_sos_data(eos_sos_data_test)\n",
    "\n",
    "    ## EOS & SOS train\n",
    "    eos_cols_train = [col for col in eos_sos_train.columns if 'eos' in col]\n",
    "    eos_cols_test = [col for col in eos_sos_test.columns if 'eos' in col]\n",
    "    sos_cols_train = [col for col in eos_sos_train.columns if 'sos' in col]\n",
    "    sos_cols_test = [col for col in eos_sos_test.columns if 'sos' in col]\n",
    "    eos_train = eos_sos_train[eos_cols_train]\n",
    "    eos_test = eos_sos_test[eos_cols_test]\n",
    "    sos_train = eos_sos_train[sos_cols_train]\n",
    "    sos_test = eos_sos_test[sos_cols_test]\n",
    "    \n",
    "    # print(eos[sort_pixel_year_columns(eos.columns)])\n",
    "    eos_train = eos_train[sort_pixel_year_columns(eos_train.columns)].to_numpy()\n",
    "    eos_test = eos_test[sort_pixel_year_columns(eos_test.columns)].to_numpy()\n",
    "    \n",
    "    sos_train = sos_train[sort_pixel_year_columns(sos_train.columns)].to_numpy()\n",
    "    sos_test = sos_test[sort_pixel_year_columns(sos_test.columns)].to_numpy()\n",
    "    \n",
    "    # print(sos_train)\n",
    "    \n",
    "    prcp_train = merge_eos_sos_data(prcp_data_train)\n",
    "    prcp_cols_train = [col for col in prcp_train.columns if 'annual_p' in col]\n",
    "    prcp_train = prcp_train[prcp_cols_train]#.to_numpy()\n",
    "    prcp_train = prcp_train[sort_pixel_year_columns(prcp_train.columns)].to_numpy()\n",
    "    \n",
    "    prcp_test = merge_eos_sos_data(prcp_data_test)\n",
    "    prcp_cols_test = [col for col in prcp_test.columns if 'annual_p' in col]\n",
    "    prcp_test = prcp_test[prcp_cols_test]#.to_numpy()\n",
    "    prcp_test = prcp_test[sort_pixel_year_columns(prcp_test.columns)].to_numpy()\n",
    "    \n",
    "    def merge_envir_data(df):\n",
    "        df_long = df.melt(\n",
    "            id_vars=['site_index', 'doy'],\n",
    "            var_name='variable',\n",
    "            value_name='value'\n",
    "        )\n",
    "        df_long['var_new'] =  'pixel_' + df_long['site_index'].astype(str) + '_' + df_long['variable']\n",
    "        df_wide = df_long.pivot(index='doy', columns='var_new', values='value')\n",
    "        return df_wide.reset_index()\n",
    "    # print(envir_data_train)\n",
    "    envir_data_train = merge_envir_data(envir_data_train)\n",
    "    envir_data_test = merge_envir_data(envir_data_test)\n",
    "    \n",
    "    ## T train\n",
    "    min_t_cols_train = [col for col in envir_data_train.columns if 'daily_min_t' in col]\n",
    "    max_t_cols_train = [col for col in envir_data_train.columns if 'daily_max_t' in col]\n",
    "    min_t_train = envir_data_train[min_t_cols_train]\n",
    "    max_t_train = envir_data_train[max_t_cols_train]\n",
    "    min_t_train = min_t_train[sort_pixel_year_columns(min_t_train.columns)].to_numpy()\n",
    "    max_t_train = max_t_train[sort_pixel_year_columns(max_t_train.columns)].to_numpy()\n",
    "    \n",
    "    min_t_cols_test = [col for col in envir_data_test.columns if 'daily_min_t' in col]\n",
    "    max_t_cols_test = [col for col in envir_data_test.columns if 'daily_max_t' in col]\n",
    "    min_t_test = envir_data_test[min_t_cols_test]\n",
    "    max_t_test = envir_data_test[max_t_cols_test]\n",
    "    min_t_test = min_t_test[sort_pixel_year_columns(min_t_test.columns)].to_numpy()\n",
    "    max_t_test = max_t_test[sort_pixel_year_columns(max_t_test.columns)].to_numpy()\n",
    "    \n",
    "    Li_cols = [col for col in envir_data_train.columns if 'Li' in col]\n",
    "    Li = envir_data_train[Li_cols]\n",
    "    Li = Li.iloc[:, [0]].rename(columns={Li.columns[0]: 'Li'})\n",
    "    Li_train = pd.concat([Li] * eos_train.shape[1], axis=1)\n",
    "    Li_train.columns = [f'Li_{i}' for i in range(eos_train.shape[1])]\n",
    "    Li_train = Li_train.to_numpy()\n",
    "    Li_test = pd.concat([Li] * eos_test.shape[1], axis=1)\n",
    "    Li_test.columns = [f'Li_{i}' for i in range(eos_test.shape[1])]\n",
    "    Li_test = Li_test.to_numpy()\n",
    "\n",
    "    \n",
    "    # Return everything as a dictionary\n",
    "    return {\n",
    "        'eos_train': eos_train,\n",
    "        'eos_test': eos_test,\n",
    "        'sos_train': sos_train,\n",
    "        'sos_test': sos_test,\n",
    "        'prcp_train': prcp_train,\n",
    "        'prcp_test': prcp_test,\n",
    "        'min_t_train': min_t_train,\n",
    "        'min_t_test': min_t_test,\n",
    "        'max_t_train': max_t_train,\n",
    "        'max_t_test': max_t_test,\n",
    "        'Li_train': Li_train,\n",
    "        'Li_test': Li_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fc562-6b3b-46cc-9966-4ea9fceb55d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Split year by year\n",
    "def kfold_split_years(eos_sos_data, envir_data, prcp_data, n_folds, random_seed=42):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    # Full list of years\n",
    "    all_years = np.array([2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023])\n",
    "\n",
    "    # Shuffle and split years into K folds\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "    folds = list(kf.split(all_years))\n",
    "    \n",
    "    def remove_year_cols(df, years_to_remove):\n",
    "        cols_to_drop = []\n",
    "        for year in years_to_remove:\n",
    "            cols_to_drop.extend([col for col in df.columns if str(year) in col])\n",
    "        return df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    all_fold_data = []\n",
    "\n",
    "    for fold_index, (train_idx, test_idx) in enumerate(folds):\n",
    "        train_years = all_years[train_idx]\n",
    "        test_years = all_years[test_idx]\n",
    "        \n",
    "\n",
    "        eos_sos_data_train = remove_year_cols(eos_sos_data, test_years)\n",
    "        eos_sos_data_test = remove_year_cols(eos_sos_data, train_years)\n",
    "        envir_data_train = remove_year_cols(envir_data, test_years)\n",
    "        envir_data_test = remove_year_cols(envir_data, train_years)\n",
    "        prcp_data_train = remove_year_cols(prcp_data, test_years)\n",
    "        prcp_data_test = remove_year_cols(prcp_data, train_years)\n",
    "        # Now call your split_data function\n",
    "        processed = split_data(eos_sos_data_train, eos_sos_data_test,\n",
    "                               envir_data_train, envir_data_test,\n",
    "                               prcp_data_train, prcp_data_test)\n",
    "\n",
    "        all_fold_data.append({\n",
    "            \"fold\": fold_index + 1,\n",
    "            \"train_years\": train_years,\n",
    "            \"test_years\": test_years,\n",
    "            \"data\": processed\n",
    "        })\n",
    "\n",
    "    return all_fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcc94b-f64e-4677-9a04-00b5b6fd44d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Train and test model\n",
    "def train(model_list, eos_train, sos_train, min_t_train, prcp_train, Li_train, max_t_train):\n",
    "    model_results = {}\n",
    "\n",
    "    for model_name, model_info in model_list.items():\n",
    "        # print(model_name)\n",
    "        # print(prcp_train)\n",
    "        if model_name in ['NCDD2', 'PC_NCDD2']:\n",
    "            data_list_train = {\n",
    "                'Tmaxi': max_t_train,\n",
    "                'Tmini': min_t_train,\n",
    "                'Li': Li_train,\n",
    "                'transition_dates': eos_train\n",
    "            }\n",
    "        elif model_name in ['SIAMP', 'SIAMNP']:\n",
    "            data_list_train = {\n",
    "                'Tmaxi': max_t_train,\n",
    "                'Tmini': min_t_train,\n",
    "                'Pmean': prcp_train,\n",
    "                'Li': Li_train,\n",
    "                'transition_dates': eos_train\n",
    "            }\n",
    "            # print(prcp_sub[mean_p_train_cols].to_numpy())\n",
    "        else:\n",
    "            data_list_train = {\n",
    "                'Tmini': min_t_train,\n",
    "                'Li': Li_train,\n",
    "                'transition_dates': eos_train\n",
    "            }\n",
    "        # print(sos_train)\n",
    "        def cost_function(par):\n",
    "            if not np.all(np.isfinite(par)):\n",
    "                return 1e10\n",
    "            # predicted = model_info['fun'](par, data_list_train)\n",
    "            if model_name in ['SIAM', 'SIAMN', 'SIAMNP', 'SIAMP']:\n",
    "                # print(sos_train)\n",
    "                predicted = model_info['fun'](par, sos_train, data_list_train)\n",
    "                # print(predicted)\n",
    "            else:\n",
    "                predicted = model_info['fun'](par, data_list_train)\n",
    "            if predicted is None or not np.all(np.isfinite(predicted)):\n",
    "                return 1e10\n",
    "            # print(predicted)\n",
    "            return np.sqrt(np.mean((predicted - eos_train) ** 2))\n",
    "    \n",
    "        bounds = list(zip(model_info['lower'], model_info['upper']))\n",
    "        result = dual_annealing(cost_function, bounds=bounds, maxiter=maxiteration, seed=42)\n",
    "        model_results[model_name] = {\n",
    "            'params': result.x,\n",
    "            'fun': result.fun,\n",
    "            'success': result.success,\n",
    "            'message': result.message\n",
    "        }\n",
    "    return model_results    \n",
    "from scipy.stats import pearsonr, linregress\n",
    "pixel_results = []\n",
    "\n",
    "def test(model_list, eos_test, sos_test, min_t_test, prcp_test, Li_test, max_t_test):\n",
    "    pixel_test_rmse_all_folds = {model: [] for model in model_list}\n",
    "    pixel_test_r2_all_folds = {model: [] for model in model_list}\n",
    "    pixel_test_slope_all_folds = {model: [] for model in model_list}\n",
    "    pixel_test_p_all_folds = {model: [] for model in model_list}\n",
    "    ## Test model\n",
    "    site_results = []\n",
    "    eos_test = eos_test.flatten()\n",
    "    pixel_rmse = {}\n",
    "    pixel_test_aic = {}\n",
    "    for model_name, model_info in model_list.items():\n",
    "        # print(model_list)\n",
    "        # print(model_name)\n",
    "        if model_name in ['NCDD2', 'PC_NCDD2', 'SIAMN']:\n",
    "            # print('PC_NCDD2')\n",
    "            data_list_test = {\n",
    "                'Tmaxi': max_t_test,\n",
    "                'Tmini': min_t_test,\n",
    "                'Li': Li_test,\n",
    "                'transition_dates': eos_test\n",
    "            }\n",
    "        elif model_name in ['SIAMP', 'SIAMNP']:\n",
    "            data_list_test = {\n",
    "                'Tmaxi': max_t_test,\n",
    "                'Tmini': min_t_test,\n",
    "                'Pmean': prcp_test,\n",
    "                'Li': Li_test,\n",
    "                'transition_dates': eos_test\n",
    "            }\n",
    "        else:\n",
    "            data_list_test = {\n",
    "                'Tmini': min_t_test,\n",
    "                'Li': Li_test,\n",
    "                'transition_dates': eos_test\n",
    "            }\n",
    "        params = model_results[model_name]['params']\n",
    "    \n",
    "        # predicted_test = model_info['fun'](result.x, data_list_test)\n",
    "        if model_name in ['SIAM', 'SIAMN', 'SIAMNP', 'SIAMP']:\n",
    "            predicted_test = model_info['fun'](params, sos_test, data_list_test)\n",
    "            # print(params)\n",
    "            # print(predicted_test)\n",
    "        else:\n",
    "            predicted_test = model_info['fun'](params, data_list_test)\n",
    "        # print(predicted_test)\n",
    "        shape = predicted_test.shape  # or len(arr)\n",
    "        nan_count = np.isnan(predicted_test).sum()\n",
    "        # print(\"Shape:\", shape)\n",
    "        # print(\"Number of NaNs:\", nan_count)\n",
    "        # print(predicted_test)\n",
    "        valid_mask = (~np.isnan(eos_test)) & (~np.isnan(predicted_test))\n",
    "        # print(eos_test)\n",
    "        # print(predicted_test)\n",
    "        if np.any(valid_mask):\n",
    "            obs = eos_test[valid_mask]\n",
    "            pred = predicted_test[valid_mask]\n",
    "            residuals = pred - obs\n",
    "            if np.array_equal(pred, obs):\n",
    "                print(\"Arrays are identical:\", pred, obs)\n",
    "\n",
    "            rmse = np.sqrt(np.mean(residuals ** 2))\n",
    "            # r, p_value = pearsonr(pred, obs)\n",
    "            if np.std(pred) == 0 or np.std(obs) == 0:\n",
    "                # print(predictor1)\n",
    "                # print(model_name, obs, pred)\n",
    "                # print(observed_DOY_test, predicted_test)\n",
    "                r2 = np.nan\n",
    "                slope = np.nan\n",
    "            else:\n",
    "                slope, intercept, r_value, p_val, std_err = linregress(pred, obs)\n",
    "                r2 = r_value**2\n",
    "        else:\n",
    "            rmse, r2, slope, p_val = np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        pixel_test_rmse_all_folds[model_name].append(rmse)\n",
    "        pixel_test_r2_all_folds[model_name].append(r2)\n",
    "        pixel_test_slope_all_folds[model_name].append(slope)\n",
    "        pixel_test_p_all_folds[model_name].append(p_val)\n",
    "\n",
    "    avg_rmse = {k: np.nanmean(v) for k, v in pixel_test_rmse_all_folds.items()}\n",
    "    avg_r2 = {k: np.nanmean(v) for k, v in pixel_test_r2_all_folds.items()}\n",
    "    avg_slope = {k: np.nanmean(v) for k, v in pixel_test_slope_all_folds.items()}\n",
    "    avg_p = {k: np.nanmean(v) for k, v in pixel_test_p_all_folds.items()}\n",
    "\n",
    "    pixel_results.append({\n",
    "        \"fold_index\": float(fold_num),\n",
    "        'test_rmse': avg_rmse,\n",
    "        'test_r2': avg_r2,\n",
    "        'test_slope': avg_slope,\n",
    "        'test_p': avg_p\n",
    "    })\n",
    "    return pixel_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a2ac1-fa9f-46e0-82b1-27191445aec0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Define models dictionary\n",
    "model_list = {\n",
    "    'CDD': {'fun': CDD_model, 'init': [20, -20], 'lower': [10, -5000], 'upper': [40, 0]},\n",
    "    'DM': {'fun': DM_model, 'init': [20, 15, -20], 'lower': [10, 10, -5000], 'upper': [40, 16, 0]},\n",
    "    'SIAM': {'fun': SIAM_model, 'init': [20, 15, 20, 0], 'lower': [10, 10, 0, -1], 'upper': [40, 16, 5000, 1]},\n",
    "    'SIAMN': {'fun': SIAMN_model, 'init': [20, 15, 2, 20, 0], 'lower': [10, 10, 0.1, 0.1, -1], 'upper': [40, 16, 8, 500, 1]},  \n",
    "    'SIAMNP': {'fun': SIAMNP_model, 'init': [20, 12, 0.5, 20, 0, 2, 1], 'lower': [15, 10, 0.01, 1, -1, 1, 0.05], 'upper': [30, 16, 2, 500, 1, 10, 4]},  \n",
    "}\n",
    "\n",
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "maxiteration = 100\n",
    "eos_sos_data, envir_data, prcp_data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b3d6721-860d-4d76-9c6a-fe866f705a66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run models\n",
    "n_folds = 8 ## Not include year 2024\n",
    "\n",
    "all_fold_data = kfold_split_years(eos_sos_data, envir_data, prcp_data, n_folds)\n",
    "results = []\n",
    "# print(all_fold_data)\n",
    "for fold_info in all_fold_data:\n",
    "    # print(fold_info)\n",
    "    fold_num = fold_info[\"fold\"]\n",
    "    train_years = fold_info[\"train_years\"]\n",
    "    test_years = fold_info[\"test_years\"]\n",
    "    data = fold_info[\"data\"]\n",
    "\n",
    "    # Example: Access specific data pieces\n",
    "    eos_train = data[\"eos_train\"]\n",
    "    sos_train = data[\"sos_train\"]\n",
    "    min_t_train = data[\"min_t_train\"]\n",
    "    prcp_train = data[\"prcp_train\"]\n",
    "    Li_train = data[\"Li_train\"]\n",
    "    max_t_train = data[\"max_t_train\"]\n",
    "\n",
    "    model_results = train(model_list, eos_train, sos_train, min_t_train, prcp_train, Li_train, max_t_train)\n",
    "\n",
    "    eos_test = data[\"eos_test\"]\n",
    "    sos_test = data[\"sos_test\"]\n",
    "    min_t_test = data[\"min_t_test\"]\n",
    "    prcp_test = data[\"prcp_test\"]\n",
    "    Li_test = data[\"Li_test\"]\n",
    "    max_t_test = data[\"max_t_test\"]\n",
    "    # print(eos_test)\n",
    "    site_results = test(model_list, eos_test, sos_test, min_t_test, prcp_test, Li_test, max_t_test)\n",
    "    results.append(site_results)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44b8bea0-b6cb-4a65-8f99-e450c39f3757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.to_pickle(results[0], \"..\\\\data\\\\tables\\\\params\\\\spruce\\\\evaluation_results_spruce.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d029a-44c1-41f3-9f3f-b4b1132ec7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
